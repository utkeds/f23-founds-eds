owid_energy %>%
filter(year > 1984,
country == "United States") %>%
ggplot(aes(x = per_capita_electricity, y = year)) +
geom_line()
View(owid_energy)
owid_energy %>%
filter(year > 1984,
country == "United States") %>%
group_by(year) %>%
ggplot(aes(x = per_capita_electricity, y = year)) +
geom_line()
owid_energy %>%
filter(year > 1984,
country == "United States") %>%
group_by(year) %>%
ggplot(aes(y = per_capita_electricity, x = year)) +
geom_line()
owid_energy %>%
filter(year > 1984,
country == "United States") %>%
ggplot(aes(y = per_capita_electricity, x = year)) +
geom_line()
library(tidyverse)
library(readxl)
library(tigris)
library(janitor)
ed_race <- read_excel("data_2017_educator_race_ethnicity_by_district.xlsx")
here::her()
here::here()
ed_race <- read_excel("week-12/data/data_2017_educator_race_ethnicity_by_district.xlsx")
schl_direct <- read_excel(""week-12/data/schl_directory_2017-18.xlsx")
schl_direct <- read_excel("week-12/data/schl_directory_2017-18.xlsx")
clean_race <- clean_names(ed_race)
clean_schl <- clean_names(schl_direct)
joined_school <- left_join(clean_schl, clean_race, join_by("LEA_ID" == "district_number"))
View(clean_schl)
tn_dist <- school_districts("Tennessee")
tn_data <- left_join(tn_dist, joined_school, join_by(GEOID == LEA_NCES))
joined_school <- left_join(clean_schl, clean_race, join_by("lea_id" == "district_number"))
tn_dist <- school_districts("Tennessee")
View(clean_schl)
View(joined_school)
schl_direct <- read_excel(here::here("week-12", "data", "schl_directory_2017-18.xlsx")
schl_direct <- read_excel(here::here("week-12", "data", "schl_directory_2017-18.xlsx"))
schl_direct <- read_excel(here::here("week-12", "data", "schl_directory_2017-18.xlsx"))
library(tidyverse)
library(readxl)
library(tigris)
library(janitor)
ed_race <- read_excel(here::here("week-12", "data", "data_2017_educator_race_ethnicity_by_district.xlsx")
ed_race <- read_excel(here::here("week-12", "data", "data_2017_educator_race_ethnicity_by_district.xlsx"))
schl_direct <- read_excel(here::here("week-12", "data", "schl_directory_2017-18.xlsx"))
clean_race <- clean_names(ed_race)
View(clean_race)
clean_schl <- clean_names(schl_direct)
View(clean_schl)
??unnest_tokens
library(tidytext)
state_by_region <- read_csv("https://github.com/cphalpert/census-regions/blob/master/us%20census%20bureau%20regions%20and%20divisions.csv?raw=true") # data on U.S. states' regions
library(tidyverse)
??text_df_unnest
text_df = tibble(
text = c(
"This speech is my recital, I think it's very vital",
"To rock (a rhyme), that's right (on time)",
"It's Tricky is the title, here we go..."
)
)
tidy_df_unnest <- text_df %>%
unnest_tokens(word, text)
tidy_df_unnest %>%
head(3)
text_df_unnest %>%
anti_join(stop_words)
tidy_df_unnest %>%
anti_join(stop_words)
text_df = tibble(
text = c(
"This speech is my recital, I think it's very vital",
"To rock (a rhyme), that's right (on time)",
"It's Tricky is the title, here we go..."
)
)
text_df
text = c(
"This speech is my recital, I think it's very vital",
"To rock (a rhyme), that's right (on time)",
"It's Tricky is the title, here we go..."
)
text
text_df <- tibble(line = 1:3, text = text)
text_df
tidy_df_unnest <- text_df %>%
unnest_tokens(word, text)
tidy_df_unnest %>%
head(3)
tidy_df_unnest %>%
anti_join(stop_words)
transcript <- read_csv("data/processed-transcript.csv")
transcript <- read_csv("week-13/data/processed-transcript.csv")
transcript %>%
unnest_tokens(word, text) %>%
count(word) %>%
arrange(desc(n)) %>%
anti_join(stop_words)
word_count %>%
slice(1:10) %>%
ggplot(aes(x = reorder(word, -n), y = n, fill = n)) +
geom_col()+
xlab("Word") + ylab("Frequency")
mean_utterance_duration_and_n_utterances <- transcript %>%
mutate(utterance_duration = end-start) %>%
group_by(name) %>%
summarize(mean_utterance_duration = mean(utterance_duration),
n_utterances = n()) %>%
arrange(desc(mean_utterance_duration))
mean_utterance_duration_and_n_utterances %>%
ggplot(aes(x = reorder(name, mean_utterance_duration), y = mean_utterance_duration, fill = n_utterances)) +
geom_col() + coord_flip() +
xlab("Speaker") + ylab("Average Utterance Duration")
isabella_data <- transcript %>%
filter(userName == "Isabella Velásquez") %>%
unnest_tokens(word, text) %>%
count(word) %>%
arrange(desc(n)) %>%
anti_join(stop_words)
library(lubridate)
isabella_data_1 <- transcript %>%
filter(userName == "Isabella Velásquez") %>%
unnest_tokens(word, text) %>%
filter(word == "data") %>%
mutate(hour = hour(start),
minute = minute(start))
isabella_data <- transcript %>%
filter(userName == "Isabella Velásquez") %>%
unnest_tokens(word, text) %>%
count(word) %>%
arrange(desc(n)) %>%
anti_join(stop_words)
View(transcript)
isabella_data <- transcript %>%
filter(name == "Isabella Velásquez") %>%
unnest_tokens(word, text) %>%
count(word) %>%
arrange(desc(n)) %>%
anti_join(stop_words)
View(isabella_data)
isabella_data_1 <- transcript %>%
filter(name == "Isabella Velásquez") %>%
unnest_tokens(word, text) %>%
filter(word == "data") %>%
mutate(hour = hour(start),
minute = minute(start))
View(isabella_data_1)
data_count <- isabella_data_1 %>%
group_by(time) %>%
count() %>%
summarize(avg_um_utterance_min = mean(n))
View(isabella_data_1)
#| echo: true
tidy_df_unnest %>%
count(word, sort = TRUE)
#| echo: true
tidy_df_unnest %>%
anti_join(stop_words)
#| echo: true
tidy_df_unnest %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
my_sentences <- tibble(sentences = c("I think that she's really good at playing soccer, but he's probably a better dancer, especially at his age", "I really like to watch soccer games, especially for OneKnox", "She was a great goalkeeper, perhaps because she played soccer from such a young age"))
library(tidyverse)
my_sentences <- tibble(sentences = c("I think that she's really good at playing soccer, but he's probably a better dancer, especially at his age", "I really like to watch soccer games, especially for OneKnox", "She was a great goalkeeper, perhaps because she played soccer from such a young age"))
my_sentences %>%
unnest_tokens(word, text) %>% # separate words, removing punctuation
count(word) %>% # counting words
arrange(desc(n)) %>% # arranging by frequency
anti_join(stop_words) # removing common words, or "stop" words
library(tidytext)
my_sentences %>%
unnest_tokens(word, text) %>% # separate words, removing punctuation
count(word) %>% # counting words
arrange(desc(n)) %>% # arranging by frequency
anti_join(stop_words) # removing common words, or "stop" words
my_sentences %>%
unnest_tokens(word, sentences) %>% # separate words, removing punctuation
count(word) %>% # counting words
arrange(desc(n)) %>% # arranging by frequency
anti_join(stop_words) # removing common words, or "stop" words
my_sentences %>%
unnest_tokens(word, sentences)
install.packages('janeaustenr')
install.packages("janeaustenr")
library(forcats)
data <- data.frame(
month = c("Jan", "Feb", "Mar", "Jan", "Mar", "Feb", "Mar", "Jan", "Feb", "Feb")
)
data <- tibble(
month = c("Jan", "Feb", "Mar", "Jan", "Mar", "Feb", "Mar", "Jan", "Feb", "Feb")
)
# Convert 'month' to a factor and reorder by frequency
data <- data %>%
mutate(month = as_factor(month) %>% fct_infreq())
library(dplyr)
data <- tibble(
month = c("Jan", "Feb", "Mar", "Jan", "Mar", "Feb", "Mar", "Jan", "Feb", "Feb"))
# Convert 'month' to a factor and reorder by frequency
data <- data %>%
mutate(month = as_factor(month) %>% fct_infreq())
# Now 'month' is reordered based on frequency
summary(data$month)
data <- tibble(
month = c("Jan", "Feb", "Mar", "Jan", "Mar", "Feb", "Mar", "Jan", "Feb", "Feb"))
summary(data$month)
data <- data %>%
mutate(month = as_factor(month) %>% fct_infreq())
summary(data$month)
data <- data.frame(
city = c("New York", "Los Angeles", "Chicago", "Houston", "New York", "Chicago"),
revenue = c(15000, 22000, 18000, 12000, 13500, 16000)
)
data <- data %>% mutate(city = fct_reorder(revenue, .fun = sum))
data <- data %>%
mutate(city = city %>% fct_reorder(., revenue, sum))
# Check the levels after reordering
levels(data$city)
data <- data.frame(
education = c("High School", "Bachelor's", "Some College", "Master's", "High School"))
# Reorder the levels of 'education' to a custom order
data <- data %>% mutate(education = fct_relevel(., "Some College", "Bachelor's", "Master's", "High School"))
# Reorder the levels of 'education' to a custom order
data <- data %>%  mutate(education = fct_relevel(education, "Some College", "Bachelor's", "Master's", "High School"))
# Check the levels after reordering
levels(data$education)
data <- data.frame(
education = c("High School", "Bachelor's", "Some College", "Master's", "High School"))
levels(data$education)
# Reorder the levels of 'education' to a custom order
data <- data %>%  mutate(education = fct_relevel(education, "Some College", "Bachelor's", "Master's", "High School"))
levels(data$education)
library(janeaustenr)
library(dplyr)
library(stringr)
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup()
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = fct_reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
library(ggplot2)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = fct_reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
tidy_books <- tidy_books %>%
anti_join(stop_words)
library(tidytext)
tidy_books <- original_books %>%
unnest_tokens(word, text)
tidy_books <- tidy_books %>%
anti_join(stop_words)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = fct_reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = fct_reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
library(tidyverse)
library(ggthemes)
library(rcartocolor)
library(scico)
library(janeaustenr)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidytext)
austen_books()
original_data <- austen_books()
View(original_data)
original_data <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter[\\divxlc]", ignore_case = TRUE)))) %>%
ungroup()
View(original_data)
original_data <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter[\\divxlc]", ignore_case = TRUE)))) %>%
ungroup()
View(original_data)
tidy_books <-
original_data %>%
unnest_tokens(word, text)
View(tidy_books)
View(tidy_books)
data(stop_words)
View(stop_words)
tidy_books <-
tidy_books %>%
anti_join(stop_words)
View(tidy_books)
View(tidy_books)
tidy_books %>%
count(word, sort = TRUE)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = fct_reorder(word, n)) %>%
ggplot(aes(x = n, y = word)) +
geom_col() +
labs(y = NULL)
??unnest_tokens
tidy_books <-
original_data %>%
unnest_tokens(word, text, to_lower = FALSE)
View(tidy_books)
View(stop_words)
tidy_books <-
tidy_books %>%
anti_join(stop_words)
tidy_books %>%
count(word, sort = TRUE)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = fct_reorder(word, n)) %>%
ggplot(aes(x = n, y = word)) +
geom_col() +
labs(y = NULL)
honorifics <- c("Mr", "Mrs", "Miss")
honorifics <- c("Mr", "Mrs", "Miss")
original_data <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter[\\divxlc]", ignore_case = TRUE)))) %>%
ungroup()
tidy_books <-
original_data %>%
unnest_tokens(word, text, to_lower = FALSE)
View(tidy_books)
View(tidy_books)
tidy_books <-
original_data %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
mutate(word = case_when(word %in% original_data ~ word,
.default = to_lower(word)))
??to_lower
library(stringr)
tidy_books <-
original_data %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
mutate(word = case_when(word %in% original_data ~ word,
.default = to_lower(word)))
tidy_books <-
original_data %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
mutate(word = case_when(word %in% original_data ~ word,
.default = str_to_lower(word)))
data(stop_words)
tidy_books <-
tidy_books %>%
anti_join(stop_words)
tidy_books %>%
count(word, sort = TRUE)
View(tidy_books)
honorifics <- c("Mr", "Mrs", "Miss")
tidy_books <-
original_data %>%
unnest_tokens(word, text, to_lower = FALSE)
View(tidy_books)
tidy_books <-
original_data %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
mutate(word = case_when(word %in% original_data ~ word,
.default = str_to_lower(word)))
tidy_books <-
original_data %>%
unnest_tokens(word, text, to_lower = FALSE) %>%
mutate(word = case_when(word %in% honorifics ~ word,
.default = str_to_lower(word)))
View(tidy_books)
data(stop_words)
tidy_books <-
tidy_books %>%
anti_join(stop_words)
tidy_books %>%
count(word, sort = TRUE)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = fct_reorder(word, n)) %>%
ggplot(aes(x = n, y = word)) +
geom_col() +
labs(y = NULL)
mean_utterance_duration_and_n_utterances <- transcript %>%
mutate(utterance_duration = end-start) %>%
group_by(name) %>%
summarize(mean_utterance_duration = mean(utterance_duration),
n_utterances = n()) %>%
arrange(desc(mean_utterance_duration))
library(tidytext)
library(tidyverse)
transcript <- read_csv("week-13/data/processed-transcript.csv")
transcript %>%
unnest_tokens(word, text) %>%
count(word) %>%
arrange(desc(n)) %>%
anti_join(stop_words)
word_count %>%
slice(1:10) %>%
ggplot(aes(x = reorder(word, -n), y = n, fill = n)) +
geom_col()+
xlab("Word") + ylab("Frequency")
mean_utterance_duration_and_n_utterances <- transcript %>%
mutate(utterance_duration = end-start) %>%
group_by(name) %>%
summarize(mean_utterance_duration = mean(utterance_duration),
n_utterances = n()) %>%
arrange(desc(mean_utterance_duration))
View(mean_utterance_duration_and_n_utterances)
mean_utterance_duration_and_n_utterances <- transcript %>%
mutate(
start = lubridate::seconds_to_period(hms(start)),
end = lubridate::seconds(hms(end)),
utterance_duration = end - start
) %>%
group_by(name) %>%
summarize(mean_utterance_duration = mean(utterance_duration),
n_utterances = n()) %>%
arrange(desc(mean_utterance_duration))
View(mean_utterance_duration_and_n_utterances)
View(mean_utterance_duration_and_n_utterances)
View(mean_utterance_duration_and_n_utterances)
install.packages('topicmodels')
library(topicmodels)
data("AssociatedPress", package = "topicmodels")
AssociatedPress
lda_model <- LDA(AssociatedPress, k = 10)
topics <- terms(lda_model, 6)
print(topics)
forums_dtm <- read_rds("https://github.com/utkeds/f23-founds-eds/blob/main/data/forums-dtm.rds?raw=true")
library(tidyverse)
forums_dtm <- read_rds("https://github.com/utkeds/f23-founds-eds/blob/main/data/forums-dtm.rds?raw=true")
library(topicmodels)
data("AssociatedPress")
View(AssociatedPress)
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_lda
library(tidytext)
ap_topics <- tidy(ap_lda, matrix = "beta")
library(tidyverse)
ap_topics <- tidy(ap_lda, matrix = "beta")
library(reshape2)
install.packages('reshape2')
ap_topics <- tidy(ap_lda, matrix = "beta")
View(ap_topics)
View(ap_topics)
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
View(ap_top_terms)
??reorder_within
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scale = "free") +
scale_y_reordered()
beta_wide <- ap_topics %>%
mutate(topic = paste0("topic", topic)) %>%
pivot_wider(names_from = topics, values_from = beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
beta_wide <- ap_topics %>%
mutate(topic = paste0("topic", topic)) %>%
pivot_wider(names_from = topic, values_from = beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
View(beta_wide)
library(reshape2)
remove.packages("reshape2")
library(topicmodels)
library(tidyverse)
library(topicmodels)
library(tidyverse)
library(tidytext)
data("AssociatedPress")
ap_topics <- tidytext::tidy(ap_lda, matrix = "beta")
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_topics <- tidytext::tidy(ap_lda, matrix = "beta")
