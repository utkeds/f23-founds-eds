<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Week 14 Slides</title>
    <meta charset="utf-8" />
    <meta name="author" content="Isabella Velásquez and Maryrose Weatherton" />
    <meta name="date" content="2023-11-20" />
    <script src="week-14-slides_files/header-attrs-2.24/header-attrs.js"></script>
    <link href="week-14-slides_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="week-14-slides_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <script src="week-14-slides_files/clipboard-2.0.6/clipboard.min.js"></script>
    <link href="week-14-slides_files/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="week-14-slides_files/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
    <link href="week-14-slides_files/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
    <link href="week-14-slides_files/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />
    <link href="week-14-slides_files/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="week-14-slides_files/panelset-0.2.6/panelset.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Week 14 Slides
]
.subtitle[
## Topic Modeling
]
.author[
### <strong>Isabella Velásquez and Maryrose Weatherton</strong>
]
.date[
### November 20, 2023
]

---










# Purpose and Agenda

This week, we'll continue our exploration of text analysis specifically with topic modeling. Topic modeling is a method for unsupervised classification of such text---meaning no codes or themes for documents are needed in advance, It similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.

## What we'll do in this presentation

- Discussion
- Key Concept: Topic modeling
- Code-along and data
- Discussion
- What's next: Assignment(s) and Readings

---

## TNVoice

TNVoice (TN Volunteer Online Instructor &amp; Course Evaluations) will open to students on Monday, November 20 and will remain open until 11:59 on Wednesday, December 6.

- Should be available in your email or on Canvas
- Can help provide evidence of the effectiveness of the class in future CVs/materials

---

# Importing data in other formats

* Google Sheets: googlesheets4 provides an R interface to Google Sheets via the Sheets API v4.
* Microsoft Word: docxtractr (for tables in Word), readtext, officer
* PDFs: pdftools, tabulizer
* SAS/SPSS/Stata: the haven package reads in files of various formats

---

# Discussion

.panelset[

.panel[.panel-name[Definition]

Topic modeling

- A technique used in natural language processing and machine learning 
- Used to automatically identify and extract hidden topics or themes from a collection of documents or text data
- Discover the underlying structure or patterns in a large corpus of text without prior knowledge of what those topics might be

]

.panel[.panel-name[Background]

- Traditionally, something like what topic modeling can produce has been achieved through _thematic, qualitative analysis_ of text data
- However, the power of computers --- even your own computer --- can be leveraged to achieve something like what thematic, qualitative analysis can 
- Such computer-driven analyses go by many names that refer to similar but distinct techniques, including: 
    - natural language processing (NLP)
    - computational text analysis
    - computational linguistics
    - text mining

]

.panel[.panel-name[Discussion Question]

- What are some ways you have explored or described the themes in text?
- What do you see as the potential of analyzing text with your computer?

]
]

---

# Prepping your data

.panelset[

.panel[.panel-name[Data Collection and Cleaning]

- Gathering the text that you want to analyze
- Handling missing data
- Correcting typos or errors
- Standardizing text format (e.g., lowercasing all text)
- Text tokenization
- Stopword removal

&lt;img src="tmwr_0101.png" width="100%" style="display: block; margin: auto;" /&gt;

]

.panel[.panel-name[Exploratory Data Analysis (EDA)]

- Basic statistics: number of documents and the total number of words
- Word frequency analysis
- Word clouds and other visualizations

]
]

---

# Key Concept: Topic modeling

.panelset[

.panel[.panel-name[LDA and topic models]

- Latent Dirichlet allocation (LDA) is a method for fitting a topic model.
    - Each document is a mixture of topics, and each topic is a mixture of words; this means that documents share similarities based on the distribution of topics.
    - Words can be shared between topics; this means that topics may overlap in their word distributions, indicating thematic similarities.

]

.panel[.panel-name[*k*]

- A key aspect of topic modeling is determining *k*, the number of topics.
- Choosing the right *k* is critical for model accuracy and interpretability.
- Various methods, help in selecting an appropriate *k*, but **there's no one criterion used to select k**, and often analysts consider a variety of solutions with different numbers of topics (different *k*s)

]

.panel[.panel-name[Interpretation]

- The interpretation of topic models is a non-trivial step.
- It involves understanding the thematic substance of each topic based on the distribution of words.
- Effective interpretation requires domain knowledge and sometimes iterative refinement of the model.

]
]

---

# Key Concept: Document-term matrix

.panelset[

.panel[.panel-name[Document-term matrix]

**Document-term matrix (DTM)**: Transform your tokenized and preprocessed text data into a DTM. 

- Each row represents one document (such as a book or article)
- Each column represents one term, and
- Each value (typically) contains the number of appearances of that term in that document

]

.panel[.panel-name[Converting to and from non-tidy formats]

- A DTM is typically comparable to a tidy data frame after a count or a group_by/summarize that contains counts or another statistic for each combination of a term and document.
- DTM objects cannot be used directly with tidy tools.
- The tidytext package provides two verbs that convert between the two formats:
    - `tidy()` turns a document-term matrix into a tidy data frame
    - `cast()` turns a tidy one-term-per-row data frame into a matrix

]
]

---

# Key Concept: The topicmodels package

.panelset[

.panel[.panel-name[topicmodels]

The topicmodels package provides basic infrastructure for fitting topic models based on data structures

]

.panel[.panel-name[AssociatedPress]

- Here's a snippet showing how you can use the topicmodels package with the AssociatedPress dataset, an example of a DocumentTermMatrix:


```r
library(topicmodels)

data("AssociatedPress", package = "topicmodels")
AssociatedPress
```

```
## &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt;
## Non-/sparse entries: 302031/23220327
## Sparsity           : 99%
## Maximal term length: 18
## Weighting          : term frequency (tf)
```

]

.panel[.panel-name[Fitting the model]

We can use the `LDA()` function from the topicmodels package, setting k = 10, to create a ten-topic LDA model.


```r
lda_model &lt;- LDA(AssociatedPress, k = 10, control = list(seed = 1234))
lda_model
```

```
## A LDA_VEM topic model with 10 topics.
```

.panel[.panel-name[Topics]

Using `terms()`, we can get the following results:

- **Topic Labels:** The numbers at the top represent the topics generated by the LDA model, ranging from Topic 1 to Topic 10. These labels are typically generated automatically by the model.
- **Top Words in Each Topic:** Each row under the topic labels lists the top words associated with that topic.
- **Interpreting Topics:** To interpret the topics, you can examine the most significant words in each topic and try to identify common themes or subjects.

]

.panel[.panel-name[Topics]


```r
topics &lt;- terms(lda_model, 6)
print(topics)
```

```
##      Topic 1   Topic 2     Topic 3   Topic 4     Topic 5      Topic 6   
## [1,] "percent" "bush"      "million" "police"    "government" "police"  
## [2,] "market"  "states"    "company" "military"  "party"      "people"  
## [3,] "prices"  "house"     "billion" "two"       "people"     "i"       
## [4,] "year"    "president" "year"    "people"    "political"  "two"     
## [5,] "new"     "united"    "new"     "officials" "south"      "city"    
## [6,] "dollar"  "bill"      "corp"    "army"      "president"  "children"
##      Topic 7      Topic 8     Topic 9   Topic 10
## [1,] "office"     "i"         "court"   "soviet"
## [2,] "department" "dukakis"   "case"    "west"  
## [3,] "new"        "bush"      "judge"   "east"  
## [4,] "federal"    "new"       "drug"    "united"
## [5,] "students"   "president" "trial"   "german"
## [6,] "school"     "campaign"  "federal" "union"
```

]
]

---

## Interpreting models

- Topic modeling is very powerful: you can estimate the topics that describe 100s, 1000s, or 10,000+ documents quickly.
- But, it is not a panacea, and it is often important to note that the topics identified through topic modeling _may not validly characterize what is in the documents_; carefully reviewing the topics, qualitatively reading sample documents to see how well the topics in fact describe them, and thoughtfully reporting results is necessary.

---

# Code-along

.panelset[

.panel[.panel-name[Loading package and data]

- Let's start by examining topics from the AssociatedPress dataset included with the topicmodels package.
- First, install and load the topicmodels package. Then, load the AssociatedPress dataset.


```r
# install.packages("topicmodels")
library(topicmodels)

data("AssociatedPress")
```

**A key thing to note here is the format of this data; when you use your own data, getting it into the right "form"--a document-term matrix--is essential. More on how to do that is [here](https://www.tidytextmining.com/topicmodeling).**

]

.panel[.panel-name[LDA()]

- Let's set the number of topics we want with the `LDA()` function
- `k` is the number of topics we want to find
- We use the code `control = list(seed = 1234)` to set a seed so that the output of the model is predictable


```r
ap_lda &lt;- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_lda
```

]

.panel[.panel-name[Word-topic probabilities]

- We need to find the per-topic-per-word probabilities of our dataset. 
- This will break our dataset into one-topic-per-term-per-row format, where the probability listed is the probability of a term being generated in each topic.
- This method is from the tidytext package.


```r
library(tidytext)

ap_topics &lt;- tidy(ap_lda, matrix = "beta")
ap_topics
```

]

.panel[.panel-name[Finding top terms]

- We can use `slice_max()` from dplyr to find the top 10 terms for each topics
- This will be a tidy data frame, which then allows us to use ggplot2 to visualize our topics.


```r
library(tidyverse)

ap_top_terms &lt;- ap_topics %&gt;%
  group_by(topic) %&gt;%
  slice_max(beta, n = 10) %&gt;% 
  ungroup() %&gt;%
  arrange(topic, -beta)

ap_top_terms %&gt;%
  mutate(term = reorder_within(term, beta, topic)) %&gt;%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

- How would you name each of these topics? Let's talk through each of them.

]
]

---

# What's next?

.panelset[

.panel[.panel-name[Weekly Assignment]

- In this week's assignment, you will perform topic modeling on a dataset of your choosing, preferably from your own research.
- It is likely you will want to prepare and tidy your dataset before starting the topic modeling process. Don't forget the skills you have used through this point in the semester---they are just as relevant now!!
- Lastly, using your own data may be hard; you shouldn't look for perfect or even very good output, instead celebrating any results/output you can produce with their data! That's what we're looking for from this assignment.
    - There's also a backup dataset you can use if you don't have one - details in the assignment doc on how they can read it directly in from GitHub.
    
]

.panel[.panel-name[Final Projects]

- How are you doing? Your final project _presentation_ is coming up soon! Please see details [here](https://utk.instructure.com/courses/184613/assignments/1607692)
- Next week, we'll have time reserved to prepare for your final project presentations (and _reports_ due after your _presentations_)

]

.panel[.panel-name[Readings]

**Substantive reading(s):**

&gt; Nelson, L. K., Burk, D., Knudsen, M., &amp; McCall, L. (2021). The future of coding: A comparison of hand-coding and three types of computer-assisted text analysis methods. Sociological Methods &amp; Research, 50(1), 202-237.

*This is a phenomenal introduction not only to topic modeling, but also other computational text analysis methods. Also, refer back to the reading from last week for a nice example of how to write up a topic modeling study!*

This is linked in Canvas.

**Technical reading(s):**

&gt; Topic modeling: https://www.tidytextmining.com/topicmodeling

This is also linked in Canvas.

]

.panel[.panel-name[Survey!]

If we have time left over, please use it to take the UTK Voices survey!


]

]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
